{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_fine_tuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgsczaXJlZjM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9d4d834b-0c9e-44c0-bb90-010b36e1a7fd"
      },
      "source": [
        "!pip install transformers -q\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "from torch import cuda\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, Dataset\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 778kB 5.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 35.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 43.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 46.3MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFMlmihrvJ9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# enter model name: 'bart', 'distil_bart', 't5_small', 't5_base'\n",
        "MODEL = 't5_small'\n",
        "\n",
        "if MODEL in ['t5_small', 't5_base']:\n",
        "  from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "if MODEL in ['bart', 'distil_bart']:\n",
        "  from transformers import BartTokenizer, BartForConditionalGeneration"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN4nlxH9na_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "configs = {'batch_size': 2,\n",
        "           'train_split': 0.95,\n",
        "          'epochs': {\n",
        "              'bart': 5,\n",
        "              'distil_bart':10,\n",
        "              't5_base': 10,\n",
        "              't5_small': 1\n",
        "          },\n",
        "          'lr': {\n",
        "              'bart': 1e-6,\n",
        "              'distil_bart': 5e-7,\n",
        "              't5_base': 1e-4,\n",
        "              't5_small': 1e-4\n",
        "          },\n",
        "          'seed': 42,\n",
        "          'max_review_len': 600,\n",
        "          'max_summary_len': 30 \n",
        "          }\n",
        "models = {\n",
        "    'bart': 'facebook/bart-large-xsum',\n",
        "    'distil_bart': 'sshleifer/distilbart-xsum-12-3',\n",
        "    't5_base': 't5-base',\n",
        "    't5_small': 't5-small'\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I67Kfb_Xl7aW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('cleaned_reviews.csv')\n",
        "df = df[['reviewText','summary']]\n",
        "if MODEL in ['t5_base', 't5_small']:\n",
        "  df.reviewText = 'summarize: ' + df.reviewText + ' </s>'\n",
        "  df.summary = df.summary + ' </s>'\n",
        "else:\n",
        "  df.reviewText = 'summarize: ' + df.reviewText\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUOPINTKmC8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wprJ1TTUq9Li",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if MODEL in ['t5_small', 't5_base']:\n",
        "  model = T5ForConditionalGeneration.from_pretrained(models[MODEL])\n",
        "  tokenizer = T5Tokenizer.from_pretrained(models[MODEL])\n",
        "if MODEL in ['bart', 'distil_bart']:\n",
        "  model = BartForConditionalGeneration.from_pretrained(models[MODEL])\n",
        "  tokenizer = BartTokenizer.from_pretrained(models[MODEL])\n",
        "  \n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j05rwi43mNEB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define a custom dataset for the dataloader\n",
        "class BookReviewsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, input_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.input_len = input_len\n",
        "        self.summ_len = summ_len\n",
        "        self.summary = self.data.summary\n",
        "        self.reviewText = self.data.reviewText\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.summary)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        reviewText = ' '.join(str(self.reviewText[index]).split())\n",
        "        summary = ' '.join(str(self.summary[index]).split())\n",
        "\n",
        "        input = self.tokenizer.batch_encode_plus([reviewText], \n",
        "                                                  max_length=self.input_len, \n",
        "                                                  pad_to_max_length=True,\n",
        "                                                  return_tensors='pt', \n",
        "                                                  truncation = True)\n",
        "        target = self.tokenizer.batch_encode_plus([summary], \n",
        "                                                  max_length = self.summ_len, \n",
        "                                                  pad_to_max_length=True,\n",
        "                                                  return_tensors='pt', \n",
        "                                                  truncation = True)\n",
        "\n",
        "        input_ids = input['input_ids'].squeeze()\n",
        "        input_mask = input['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids.to(dtype=torch.long), \n",
        "            'attn_mask': input_mask.to(dtype=torch.long), \n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "        }"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnmFoMSkmTE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    model.train()\n",
        "    for _,data in enumerate(loader, 0):\n",
        "        target_ids = data['target_ids'].to(device, dtype = torch.long)\n",
        "        decoder_input_ids = target_ids[:, :-1].contiguous()\n",
        "        lm_labels = target_ids[:, 1:].clone().detach()\n",
        "        lm_labels[target_ids[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        input_ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        attn_mask = data['attn_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids = input_ids, \n",
        "                        attention_mask = attn_mask, \n",
        "                        decoder_input_ids=decoder_input_ids, \n",
        "                        lm_labels=lm_labels)\n",
        "        \n",
        "        loss = outputs[0]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHZWvJScocwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_predictions(epoch, tokenizer, model, device, loader):\n",
        "    model.eval()\n",
        "    model_generated_summaries = []\n",
        "    user_summaries = []\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            target_ids = data['target_ids'].to(device, dtype = torch.long)\n",
        "            input_ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            attn_mask = data['attn_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids = input_ids,\n",
        "                attention_mask = attn_mask, \n",
        "                max_length=30, \n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5, \n",
        "                length_penalty=1.8, \n",
        "                early_stopping=True\n",
        "                )\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in target_ids]\n",
        "\n",
        "            model_generated_summaries.extend(preds)\n",
        "            user_summaries.extend(target)\n",
        "    return model_generated_summaries, user_summaries"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ai2-bHNqiV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(configs['seed'])\n",
        "np.random.seed(configs['seed']) \n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "train_dataset=df.sample(frac=configs['train_split'], random_state = configs['seed']).reset_index(drop=True)\n",
        "validation_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "\n",
        "#preparing the training dataset\n",
        "training_set = BookReviewsDataset(train_dataset, tokenizer, configs['max_review_len'], configs['max_summary_len'])\n",
        "train_params = {'batch_size': configs['batch_size'],\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0}\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "\n",
        "#preparing the validation dataset\n",
        "validation_set = BookReviewsDataset(validation_dataset, tokenizer, configs['max_review_len'], configs['max_summary_len'])\n",
        "validation_params = {\n",
        "    'batch_size': configs['batch_size'],\n",
        "    'shuffle': False,\n",
        "    'num_workers': 0}\n",
        "validation_loader = DataLoader(validation_set, **validation_params)\n",
        "\n",
        "#initializing the optimizer\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=configs['lr'][MODEL])\n",
        "\n",
        "#training\n",
        "for epoch in range(configs['epochs'][MODEL]):\n",
        "    train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "#save the model to be able to load later\n",
        "model.save_pretrained('./saved_{}_model/'.format(MODEL))\n",
        "\n",
        "model_generated_summaries, user_summaries = make_predictions(epoch, tokenizer, model, device, validation_loader)\n",
        "gen_summaries_df = pd.DataFrame({'model_generated_summaries':model_generated_summaries,'user_summaries':user_summaries})\n",
        "gen_summaries_df.to_csv(MODEL+'_predictions.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy8wN89z2tbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_data(row_num):\n",
        "    print('\\n')\n",
        "    print('User Summary: ')\n",
        "    with pd.option_context('display.max_colwidth', 200):\n",
        "        print(gen_summaries_df['user_summaries'][row_num])\n",
        "    print('=============================================')\n",
        "    print(\"\\n\")\n",
        "    print('Fine Tuned Generated Summary: ')\n",
        "    with pd.option_context('display.max_colwidth', 200):\n",
        "        print(gen_summaries_df['model_generated_summaries'][row_num])\n",
        "    print('=============================================')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2BIMwCD3fzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(10):\n",
        "  display_data(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qClL4dtk3jzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}